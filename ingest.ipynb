{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from constants import CHROMA_SETTINGS\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "#Â Load environment variables\n",
    "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
    "source_directory = os.environ.get('SOURCE_DIRECTORY', 'source_documents')\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if 'text/html content not found in email' in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    # \".docx\": (Docx2txtLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        print(f\"Loading {file_path}\")\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
    "        )\n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts\n",
    "\n",
    "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
    "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
    "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
    "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
    "            # At least 3 documents are needed in a working vectorstore\n",
    "            if len(list_index_files) > 3:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asad0016\\Anaconda3\\envs\\SlackEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore\n",
      "Loading documents from source_documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents:   0%|                              | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "if does_vectorstore_exist(persist_directory):\n",
    "    # Update and store locally vectorstore\n",
    "    print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "    collection = db.get()\n",
    "    texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
    "    print(f\"Creating embeddings. May take some minutes...\")\n",
    "    db.add_documents(texts)\n",
    "else:\n",
    "    # Create and store locally vectorstore\n",
    "    print(\"Creating new vectorstore\")\n",
    "    texts = process_documents()\n",
    "    print(f\"Creating embeddings. May take some minutes...\")\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
    "db.persist()\n",
    "db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'b': 'b'}\n",
      "{'b': 'b', 'a': 'a'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def isIsomorphic(s: str, t: str) -> bool:\n",
    "        if s==t:\n",
    "            return True\n",
    "        dct = {}\n",
    "        for idx, sc in enumerate(s):\n",
    "            tc = t[idx]\n",
    "            print(dct)\n",
    "            if sc in dct and dct[sc] != tc:\n",
    "                return False\n",
    "            if sc not in dct and tc in dct.values():\n",
    "                return False\n",
    "            dct[sc] = tc\n",
    "        return True\n",
    "\n",
    "print(isIsomorphic(\"badc\", \"baba\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d {'d'}\n",
      "v {'v', 'd'}\n",
      "d {'v', 'd'}\n",
      "f {'f', 'd'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def lengthOfLongestSubstring(s: str) -> int:\n",
    "        substr = \"\"\n",
    "        slen = 0\n",
    "        sset = set()\n",
    "        for c in s:\n",
    "            plen = len(sset)\n",
    "            sset.add(c)\n",
    "            print(c, sset)\n",
    "            if len(sset) == plen: # duplicate\n",
    "                slen = plen if plen > slen else slen\n",
    "                sset = set()\n",
    "                sset.add(c)\n",
    "        return slen if slen > len(sset) else len(sset)\n",
    "\n",
    "print(lengthOfLongestSubstring(\"dvdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlackEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
